{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Depot’s Search Relevance Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our project, we would help Home Depot’s to improve their customers' shopping experience by developing a model that can accurately predict the relevance of search results.<br>\n",
    "Home depot is the equivalent of selling hardware (like Google vertically) , when we type in the search box, \"I want to build a house’’, all the tools used to build the house would show up. This is the background.<br>\n",
    "The core of this project is natural language processing, the nature of NLP is the conversion of a natural language into a computer language, so the first thing we need to understand natural language processing steps, roughly from text, Tokenize, Lemma or Stemming, stopwords, to Word_List (this is the typical text pretreatment) and then do the feature engineering, which is the text into digital, finally we can build some model to train the data and We get what we want.<br>\n",
    "The most different and interesting part of this project is that we can create different features by ourselves instead of simply calling third-party libraries, which we will explain in detail later.<br>\n",
    "In order to achieve the final goal, we first import some basic library.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    ### matrix processing library\n",
    "import pandas as pd   ### table processing library\n",
    "### model abour classifier and regression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingRegressor  \n",
    "from nltk.stem.snowball import SnowballStemmer    ### text processing about stemming\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load train.csv, test.csv,  product_descriptons.csv\n",
    "\n",
    "df_train = pd.read_csv('train.csv', encoding = \"ISO-8859-1\")  \n",
    "df_pro = pd.read_csv('product_descriptions.csv', encoding='ISO-8859-1')\n",
    "df_test = pd.read_csv('test.csv', encoding = \"ISO-8859-1\")\n",
    "# df_attr = pd.read_csv('attributes.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add product description to all table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product description will be very useful since we find that the corpus of title is too small, and we need a more detailed Description who has more corpus information to support our search, or if there are two similar products using two different words as product title, you will never be able to find them.<br>\n",
    "Product_uid here to match description.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>product_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>90 degree bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>metal l brackets</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>simpson sku able</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>simpson strong  ties</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>simpson strong tie hcc668</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                      product_title  \\\n",
       "0   1       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   4       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   5       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "3   6       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "4   7       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "\n",
       "                 search_term  \\\n",
       "0          90 degree bracket   \n",
       "1           metal l brackets   \n",
       "2           simpson sku able   \n",
       "3       simpson strong  ties   \n",
       "4  simpson strong tie hcc668   \n",
       "\n",
       "                                 product_description  \n",
       "0  Not only do angles make joints stronger, they ...  \n",
       "1  Not only do angles make joints stronger, they ...  \n",
       "2  Not only do angles make joints stronger, they ...  \n",
       "3  Not only do angles make joints stronger, they ...  \n",
       "4  Not only do angles make joints stronger, they ...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### for text processing, concat train file and product description file.\n",
    "\n",
    "df_all = pd.merge(df_train, df_pro, how='left', on='product_uid')\n",
    "\n",
    "### for text processing, concat test file and product description file.\n",
    "df_all_test = pd.merge(df_test, df_pro, how='left', on='product_uid')\n",
    "df_all_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then let's count the relevance frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "1.00     2105\n",
       "1.25        4\n",
       "1.33     3006\n",
       "1.50        5\n",
       "1.67     6780\n",
       "1.75        9\n",
       "2.00    11730\n",
       "2.25       11\n",
       "2.33    16060\n",
       "2.50       19\n",
       "2.67    15202\n",
       "2.75       11\n",
       "3.00    19125\n",
       "Name: num, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y = pd.DataFrame(df_train['relevance'])\n",
    "df_y['num'] = 1\n",
    "df_y_g = df_y['num'].groupby(df_y['relevance'])\n",
    "df_y_g.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Conclusion: The relevance score ranges from 1 to 3, with a grade of 0.33 or 0.34. Among them, 1.25, 1.50, etc. are too few samples, which can be ignored and divided into 7 categories. The regression problem can be converted into a classification problem for consideration. The regression problem should be treated first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing for text model preformance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import libraries for stemming, remove stopwords and string similarity calculation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import library about text preprocessing\n",
    "\n",
    "from nltk import SnowballStemmer  \n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import Levenshtein  ### Library for string similarity calculation\n",
    "import nltk\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For natural language processing, we want to improve the performance of the model and greatly reduce the complexity of the text, so we do the following: first, we remove useless symbols.But we should pay attention to some of the symbols is particularly important and cannot be removed, like [\". \", \"/\", \"-\", \"%\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For improving text model performance, to process text string, \n",
    "### remove useless symbol. \n",
    "\n",
    "pattern_replace_pair_list = [\n",
    "            (r\"<.+?>\", r\"\"),\n",
    "            # html codes\n",
    "            (r\"&nbsp;\", r\" \"),\n",
    "            (r\"&amp;\", r\"&\"),\n",
    "            (r\"&#39;\", r\"'\"),\n",
    "            (r\"/>/Agt/>\", r\"\"),\n",
    "            (r\"</a<gt/\", r\"\"),\n",
    "            (r\"gt/>\", r\"\"),\n",
    "            (r\"/>\", r\"\"),\n",
    "            (r\"<br\", r\"\"),\n",
    "            # can't remove [\".\", \"/\", \"-\", \"%\"] as they are useful in numbers, \n",
    "           ### e.g., 1.97, 1-1/2, 10%, etc.\n",
    "            (r\"[ &<>)(_,;:!?\\+^~@#\\$\\*]+\", r\" \"),\n",
    "            (r\"'s\\\\b\", r\"\"),\n",
    "            (r\"[']+\", r\"\"),\n",
    "            #(r'([A-Z][a-z]+|[a-z]+|\\d+)', r'\\1 '),\n",
    "            (r'(\\d?)([a-zA-Z]+)', r'\\1 \\2 '),\n",
    "            #(r'(/d+)', r' \\1 '),\n",
    "            (r'([A-Z][a-z]+)', r' \\1 '),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some  function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we write some feature functions that we can call directly later, for example<br>\n",
    "1. Convert the number to the word<br>\n",
    "2. Lower, remove symbol stop word.<br>\n",
    "3. The stemmer<br>\n",
    "4. The feature extraction function<br>\n",
    "(1) how many words in str1 of str2<br>\n",
    "This is a function that can calculate the validity characteristics of a keyword, string1 is the title string, string2 is our search string, and we can calculate the validity of a keyword simply by calculating how many times it occurs<br>\n",
    "(2) in contrast, we can also calculate how many words are not in str1 of str2<br>\n",
    "(3) Determine how many words in the title appear in str2, str3 together (string3 is a description string)<br>\n",
    "6.Calculate similarity between two words<br>\n",
    "7.Calculate similarity between two sentences<br>\n",
    "8.Build a dictionary of stop words<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert number to word.\n",
    "\n",
    "dic = {1:'one', 2:'two', 3:'three', 4:'four', 5:'five',6:'six', 7:'seven', 8:'eight', 9:'night',0:'zero'}\n",
    "\n",
    "### \n",
    "def dashrep(matchobj):\n",
    "    if len(matchobj.group())==1:\n",
    "        return dic[int(matchobj.group())]\n",
    "    else:\n",
    "        return matchobj.group() \n",
    "\n",
    "# lower,  remove symbol stop word.\n",
    "def transform(text):\n",
    "    for pattern, replace in pattern_replace_pair_list:\n",
    "        try:\n",
    "            text = re.sub(pattern, replace, text)\n",
    "        except:\n",
    "            pass\n",
    "    #text = re.sub(r'[\\d]+', dashrep, text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return ' '.join([word for word in text.lower().split() if word not in dic_stopwords])\n",
    "\n",
    "#word_list = \"Package stopwords is already up-to-date\".split(\" \")\n",
    "#filtered_words = [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "### stemmer\n",
    "def str_stemmer(s):\n",
    "    # not stmmer, lower\n",
    "    if isinstance(s, float):\n",
    "        s = unicode(s)\n",
    "    return ' '.join([stemmer.stem(word) for word in s.split()])\n",
    "\n",
    "### Feature extraction.\n",
    "\n",
    "#  how many word in str1 of str2.\n",
    "def str_common_word(str1, str2):\n",
    "    return sum(int(str2.find(word)>=0) for word in str1.split())\n",
    "\n",
    "#  how many word not in str1 of str2.\n",
    "\n",
    "def str_notcommon_word(str1, str2):\n",
    "    return sum(int(str2.find(word)==-1) for word in str1.split())\n",
    "\n",
    "# str1:title,str2:search,str3:desc\n",
    "\n",
    "# Determine how many words in the title appear in str2, str3 together\n",
    "\n",
    "def str_common_desc_pro_word(str1, str2, str3):\n",
    "    return sum(int(str2.find(word)>=0 and str3.find(word)>=0) for word in str1.split())\n",
    "\n",
    "### Ratio between two words\n",
    "def word_vs_word_ratio(str1, str2):\n",
    "    ratio = 0\n",
    "    count = 0\n",
    "    for word1 in str1.split():\n",
    "        for word2 in str2.split():\n",
    "            ratio = Levenshtein.ratio(word1, word2)+ratio\n",
    "            count+=1\n",
    "    return ratio/max(count,1)\n",
    "\n",
    "### Ratio between search term and target term\n",
    "def search_vs_word_ratio(str_search, str_des):\n",
    "    ratio = 0\n",
    "    if len(str_search) ==0:\n",
    "        return 0\n",
    "    for word in str_des:\n",
    "        ratio = max(Levenshtein.ratio(str_search, word), ratio)\n",
    "    return ratio\n",
    "\n",
    "### Calculate similarity between two words\n",
    "def similarity(word1, word2):    \n",
    "    from nltk.corpus import wordnet as wn\n",
    "    word_1 = wn.synsets(word1)\n",
    "    word_2 = wn.synsets(word2)\n",
    "    sl = 0.\n",
    "    for el1 in word_1:\n",
    "        for el2 in word_2:\n",
    "            val = el1.path_similarity(el2)\n",
    "            if val is not None:\n",
    "                sl = max(val,sl)\n",
    "                if sl > 0.8:\n",
    "                    break\n",
    "    return sl\n",
    "\n",
    "\n",
    "### Calculate similarity between two sentences.\n",
    "def similarity_sentences(str1, str2):\n",
    "    sl, count, Ntotal, Nmatch = 0., 0., 0., 0.\n",
    "    for word1 in nltk.pos_tag(str1.split()):\n",
    "        score = 0\n",
    "        for word2 in nltk.pos_tag(str2.split()):\n",
    "            score = max(Levenshtein.ratio(word1[0],word2[0]),score)\n",
    "            if score < 0.75:\n",
    "                if word1[1][0]==word2[1][0]:\n",
    "                    score = max(similarity(word1[0], word2[0]),score)\n",
    "            #print score, word1, word2\n",
    "            if score < 0.75:\n",
    "                continue\n",
    "            sl += score\n",
    "            count += 1\n",
    "            break\n",
    "        if score > 0.7:\n",
    "            Nmatch += 1\n",
    "    if count == 0:\n",
    "        return [0., 0.]\n",
    "    return  [sl/count,Nmatch/max(len(str1.split()),1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/zwl/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "### build a dictionary of sopt words.\n",
    "nltk.download('stopwords')\n",
    "dic_stopwords = dict(zip(stopwords.words('english'),range(len(stopwords.words('english')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation, lowercase, stop words by calling above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use the above text processing and feature extraction functions to process training text data，same to test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the above text processing and feature extraction \n",
    "### functions to process training text data\n",
    "df_all['search_term_transform'] = df_all['search_term'].map(lambda x:transform(x))\n",
    "df_all['product_title_transform'] = df_all['product_title'].map(lambda x:transform(x))\n",
    "df_all['pro_des_trans'] = df_all['product_description'].map(lambda x:transform(x))\n",
    "\n",
    "### Use the above text processing and feature extraction \n",
    "### functions to process test text data\n",
    "df_all_test['search_term_transform'] = df_all_test['search_term'].map(lambda x:transform(x))\n",
    "df_all_test['product_title_transform'] = df_all_test['product_title'].map(lambda x:transform(x))\n",
    "df_all_test['pro_des_trans'] = df_all_test['product_description'].map(lambda x:transform(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming by calling above functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "By the way, stemming is very important in search, because if we're going to do similarity checking, the easiest thing to do is to calculate the validity of your keywords. <br>\n",
    "If your keywords are apples, but the text itself is an apple, it won't match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the above text processing and feature extraction functions to perform \n",
    "### stem processing on the training text data\n",
    "\n",
    "df_all['search_term_transform_stem'] = df_all['search_term_transform'].map(lambda x:str_stemmer(x))\n",
    "df_all['product_title_transform_stem'] = df_all['product_title_transform'].map(lambda x:str_stemmer(x))\n",
    "df_all['pro_des_trans_stem'] = df_all['pro_des_trans'].map(lambda x:str_stemmer(x))\n",
    "\n",
    "### Use the above text processing and feature extraction functions to perform stem processing\n",
    "### on the test text data\n",
    "df_all_test['search_term_transform_stem'] = df_all_test['search_term_transform'].map(lambda x:str_stemmer(x))\n",
    "df_all_test['product_title_transform_stem'] = df_all_test['product_title_transform'].map(lambda x:str_stemmer(x))\n",
    "df_all_test['pro_des_trans_stem'] = df_all_test['pro_des_trans'].map(lambda x:str_stemmer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we concat text words.Combine text to facilitate subsequent text feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### concat text words.Combine text to facilitate subsequent text feature extraction\n",
    "\n",
    "### concat train text.\n",
    "df_all['all_texts_transform'] = df_all['product_title_transform'] + ' . ' + df_all['pro_des_trans']\n",
    "df_all['all_texts_trans_stemm'] = df_all['product_title_transform_stem'] + ' . ' + df_all['pro_des_trans_stem']\n",
    "\n",
    "### concat test text.\n",
    "df_all_test['all_texts_transform'] = df_all_test['product_title_transform'] + ' . ' + df_all_test['pro_des_trans']\n",
    "df_all_test['all_texts_trans_stemm'] = df_all_test['product_title_transform_stem'] + ' . ' + df_all_test['pro_des_trans_stem']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>product_title</th>\n",
       "      <th>search_term</th>\n",
       "      <th>product_description</th>\n",
       "      <th>search_term_transform</th>\n",
       "      <th>product_title_transform</th>\n",
       "      <th>pro_des_trans</th>\n",
       "      <th>search_term_transform_stem</th>\n",
       "      <th>product_title_transform_stem</th>\n",
       "      <th>pro_des_trans_stem</th>\n",
       "      <th>all_texts_transform</th>\n",
       "      <th>all_texts_trans_stemm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>90 degree bracket</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>90 degree bracket</td>\n",
       "      <td>simpson strong - tie 12- gauge angle</td>\n",
       "      <td>angles make joints stronger also provide consi...</td>\n",
       "      <td>90 degre bracket</td>\n",
       "      <td>simpson strong - tie 12- gaug angl</td>\n",
       "      <td>angl make joint stronger also provid consist s...</td>\n",
       "      <td>simpson strong - tie 12- gauge angle . angles ...</td>\n",
       "      <td>simpson strong - tie 12- gaug angl . angl make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>metal l brackets</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>metal l brackets</td>\n",
       "      <td>simpson strong - tie 12- gauge angle</td>\n",
       "      <td>angles make joints stronger also provide consi...</td>\n",
       "      <td>metal l bracket</td>\n",
       "      <td>simpson strong - tie 12- gaug angl</td>\n",
       "      <td>angl make joint stronger also provid consist s...</td>\n",
       "      <td>simpson strong - tie 12- gauge angle . angles ...</td>\n",
       "      <td>simpson strong - tie 12- gaug angl . angl make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>simpson sku able</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>simpson sku able</td>\n",
       "      <td>simpson strong - tie 12- gauge angle</td>\n",
       "      <td>angles make joints stronger also provide consi...</td>\n",
       "      <td>simpson sku abl</td>\n",
       "      <td>simpson strong - tie 12- gaug angl</td>\n",
       "      <td>angl make joint stronger also provid consist s...</td>\n",
       "      <td>simpson strong - tie 12- gauge angle . angles ...</td>\n",
       "      <td>simpson strong - tie 12- gaug angl . angl make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>simpson strong  ties</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>simpson strong ties</td>\n",
       "      <td>simpson strong - tie 12- gauge angle</td>\n",
       "      <td>angles make joints stronger also provide consi...</td>\n",
       "      <td>simpson strong tie</td>\n",
       "      <td>simpson strong - tie 12- gaug angl</td>\n",
       "      <td>angl make joint stronger also provid consist s...</td>\n",
       "      <td>simpson strong - tie 12- gauge angle . angles ...</td>\n",
       "      <td>simpson strong - tie 12- gaug angl . angl make...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>100001</td>\n",
       "      <td>Simpson Strong-Tie 12-Gauge Angle</td>\n",
       "      <td>simpson strong tie hcc668</td>\n",
       "      <td>Not only do angles make joints stronger, they ...</td>\n",
       "      <td>simpson strong tie hcc 668</td>\n",
       "      <td>simpson strong - tie 12- gauge angle</td>\n",
       "      <td>angles make joints stronger also provide consi...</td>\n",
       "      <td>simpson strong tie hcc 668</td>\n",
       "      <td>simpson strong - tie 12- gaug angl</td>\n",
       "      <td>angl make joint stronger also provid consist s...</td>\n",
       "      <td>simpson strong - tie 12- gauge angle . angles ...</td>\n",
       "      <td>simpson strong - tie 12- gaug angl . angl make...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  product_uid                      product_title  \\\n",
       "0   1       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "1   4       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "2   5       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "3   6       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "4   7       100001  Simpson Strong-Tie 12-Gauge Angle   \n",
       "\n",
       "                 search_term  \\\n",
       "0          90 degree bracket   \n",
       "1           metal l brackets   \n",
       "2           simpson sku able   \n",
       "3       simpson strong  ties   \n",
       "4  simpson strong tie hcc668   \n",
       "\n",
       "                                 product_description  \\\n",
       "0  Not only do angles make joints stronger, they ...   \n",
       "1  Not only do angles make joints stronger, they ...   \n",
       "2  Not only do angles make joints stronger, they ...   \n",
       "3  Not only do angles make joints stronger, they ...   \n",
       "4  Not only do angles make joints stronger, they ...   \n",
       "\n",
       "        search_term_transform               product_title_transform  \\\n",
       "0           90 degree bracket  simpson strong - tie 12- gauge angle   \n",
       "1            metal l brackets  simpson strong - tie 12- gauge angle   \n",
       "2            simpson sku able  simpson strong - tie 12- gauge angle   \n",
       "3         simpson strong ties  simpson strong - tie 12- gauge angle   \n",
       "4  simpson strong tie hcc 668  simpson strong - tie 12- gauge angle   \n",
       "\n",
       "                                       pro_des_trans  \\\n",
       "0  angles make joints stronger also provide consi...   \n",
       "1  angles make joints stronger also provide consi...   \n",
       "2  angles make joints stronger also provide consi...   \n",
       "3  angles make joints stronger also provide consi...   \n",
       "4  angles make joints stronger also provide consi...   \n",
       "\n",
       "   search_term_transform_stem        product_title_transform_stem  \\\n",
       "0            90 degre bracket  simpson strong - tie 12- gaug angl   \n",
       "1             metal l bracket  simpson strong - tie 12- gaug angl   \n",
       "2             simpson sku abl  simpson strong - tie 12- gaug angl   \n",
       "3          simpson strong tie  simpson strong - tie 12- gaug angl   \n",
       "4  simpson strong tie hcc 668  simpson strong - tie 12- gaug angl   \n",
       "\n",
       "                                  pro_des_trans_stem  \\\n",
       "0  angl make joint stronger also provid consist s...   \n",
       "1  angl make joint stronger also provid consist s...   \n",
       "2  angl make joint stronger also provid consist s...   \n",
       "3  angl make joint stronger also provid consist s...   \n",
       "4  angl make joint stronger also provid consist s...   \n",
       "\n",
       "                                 all_texts_transform  \\\n",
       "0  simpson strong - tie 12- gauge angle . angles ...   \n",
       "1  simpson strong - tie 12- gauge angle . angles ...   \n",
       "2  simpson strong - tie 12- gauge angle . angles ...   \n",
       "3  simpson strong - tie 12- gauge angle . angles ...   \n",
       "4  simpson strong - tie 12- gauge angle . angles ...   \n",
       "\n",
       "                               all_texts_trans_stemm  \n",
       "0  simpson strong - tie 12- gaug angl . angl make...  \n",
       "1  simpson strong - tie 12- gaug angl . angl make...  \n",
       "2  simpson strong - tie 12- gaug angl . angl make...  \n",
       "3  simpson strong - tie 12- gaug angl . angl make...  \n",
       "4  simpson strong - tie 12- gaug angl . angl make...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then， we Create two pandas to store training features and test features。<br>\n",
    "we use the previous feature function to extract training text features and test text features separately<br>\n",
    "for example：<br>\n",
    "length of key words.<br>\n",
    "How many keywords coincide with search terms in the title<br>\n",
    "How many keywords coincide with the search term in the description<br>\n",
    "and the output are as follows<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_of_query</th>\n",
       "      <th>commons_in_title</th>\n",
       "      <th>commons_in_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len_of_query  commons_in_title  commons_in_desc\n",
       "0             3                 0                1\n",
       "1             3                 1                1\n",
       "2             3                 0                0\n",
       "3             3                 0                1\n",
       "4             4                 0                1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create two pandas to store training features and test features\n",
    "df_model = pd.DataFrame({})\n",
    "df_model_test = pd.DataFrame({})\n",
    "\n",
    "#3） text features.\n",
    "\n",
    "### Apply the previous feature function \n",
    "### to extract training text features and test text features separately\n",
    "\n",
    "# length of key words.\n",
    "df_model['len_of_query'] = df_all['search_term'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "df_model_test['len_of_query'] = df_all_test['search_term'].map(lambda x: len(x.split())).astype(np.int64)\n",
    "\n",
    "# How many keywords coincide with search terms in the title\n",
    "df_model['commons_in_title'] = df_all.apply(lambda x:str_common_word(x['search_term'], x['product_title']), axis=1)\n",
    "df_model_test['commons_in_title'] = df_all_test.apply(lambda x:str_common_word(x['search_term'], x['product_title']), axis=1)\n",
    "\n",
    "#How many keywords coincide with the search term in the description\n",
    "#df_all['commons_in_desc'] = df_all.apply(lambda x: str_common_word(x['search_term'], x['product_description'], axis = 1)\n",
    "df_model['commons_in_desc'] = df_all.apply(lambda x: str_common_word(x['search_term'], x['product_description']), axis=1)\n",
    "df_model_test['commons_in_desc'] = df_all_test.apply(lambda x: str_common_word(x['search_term'], x['product_description']), axis=1)\n",
    "# df_all = df_all.drop(['search_term', 'product_title', 'product_description'], axis=1)\n",
    "df_model_test.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we use the previous feature function to extract training text features, then get some new features.<br>\n",
    "For example:<br>\n",
    "search_term and product_title comparison<br>\n",
    "search_term and product_description comparison<br>\n",
    "How many keywords overlap in the product title<br>\n",
    "How many keywords overlap in the description<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### Apply the previous feature function \n",
    "### to extract training text features。\n",
    "\n",
    "df_model['len_search_term']=df_all['search_term_transform_stem'].map(\n",
    "        lambda x:len(x.split())).astype(np.int64)\n",
    "\n",
    "### new feature.\n",
    "\n",
    "# 1. search_term and product_title comparison\n",
    "df_model['dist_in_title'] = df_all.apply(lambda x:word_vs_word_ratio((x['search_term_transform_stem']),x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "df_model['dist_in_title1'] = df_all.apply(lambda x:search_vs_word_ratio((x['search_term_transform_stem']),x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "# search_term and product_description comparison\n",
    "df_model['dist_in_desc'] = df_all.apply(lambda x:word_vs_word_ratio((x['search_term_transform_stem']),x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model['dist_in_desc1'] = df_all.apply(lambda x:search_vs_word_ratio((x['search_term_transform_stem']),x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model['len_of_query']=df_all['search_term_transform_stem'].map(\n",
    "        lambda x:len(x.split())).astype(np.int64)\n",
    "df_model['len_search'] = df_all['search_term_transform_stem'].map(lambda x:len(x))\n",
    "\n",
    "# How many keywords overlap in the product title\n",
    "df_model['commons_in_title']=df_all.apply(\n",
    "    lambda x:str_common_word(\n",
    "    x['search_term_transform_stem'],x['product_title_transform_stem']),axis=1)\n",
    "\n",
    "# How many keywords overlap in the description\n",
    "df_model['commons_in_desc'] = df_all.apply(lambda x:str_common_word(x['search_term_transform_stem'], x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model['common_in_desc_pro']=df_all.apply(lambda x: str_common_desc_pro_word(x['search_term_transform_stem'],x['pro_des_trans_stem'],x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "# df_model['nn_word_in_search'] = df_all['search_term_transform_stem'].map(nn_word_numbers_In_Search)\n",
    "\n",
    "df_model['queryvstitle'] = df_model.apply(lambda x: float(x['commons_in_title'])/max((x['len_of_query']),1), axis=1)\n",
    "\n",
    "df_model['product_uid'] =df_all['product_uid']\n",
    "\n",
    "df_model['queryvsdesc'] = df_model.apply(lambda x: float(x['commons_in_desc'])/max(x['len_of_query'],1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we use the previous feature function to extract test text features, then get some new features.<br>\n",
    "For example:<br>\n",
    "search_term and product_title comparison<br>\n",
    "search_term and product_description comparison<br>\n",
    "How many keywords overlap in the product title<br>\n",
    "How many keywords overlap in the description<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "### Apply the previous feature function \n",
    "### to extract test text features。\n",
    "\n",
    "df_model_test['len_search_term']=df_all_test['search_term_transform_stem'].map(\n",
    "        lambda x:len(x.split())).astype(np.int64)\n",
    "\n",
    "### new feature.\n",
    "\n",
    "# 1. search_term and product_title comparison\n",
    "df_model_test['dist_in_title'] = df_all_test.apply(lambda x:word_vs_word_ratio((x['search_term_transform_stem']),x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "df_model_test['dist_in_title1'] = df_all_test.apply(lambda x:search_vs_word_ratio((x['search_term_transform_stem']),x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "# search_term and product_description comparison\n",
    "df_model_test['dist_in_desc'] = df_all_test.apply(lambda x:word_vs_word_ratio((x['search_term_transform_stem']),x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model_test['dist_in_desc1'] = df_all_test.apply(lambda x:search_vs_word_ratio((x['search_term_transform_stem']),x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model_test['len_of_query']=df_all_test['search_term_transform_stem'].map(\n",
    "        lambda x:len(x.split())).astype(np.int64)\n",
    "df_model_test['len_search'] = df_all_test['search_term_transform_stem'].map(lambda x:len(x))\n",
    "\n",
    "# How many keywords overlap in the product title\n",
    "df_model_test['commons_in_title']=df_all_test.apply(\n",
    "    lambda x:str_common_word(\n",
    "    x['search_term_transform_stem'],x['product_title_transform_stem']),axis=1)\n",
    "\n",
    "# How many keywords overlap in the description\n",
    "df_model_test['commons_in_desc'] = df_all_test.apply(lambda x:str_common_word(x['search_term_transform_stem'], x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model_test['common_in_desc_pro']=df_all_test.apply(lambda x: str_common_desc_pro_word(x['search_term_transform_stem'],x['pro_des_trans_stem'],x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "# df_model['nn_word_in_search'] = df_all['search_term_transform_stem'].map(nn_word_numbers_In_Search)\n",
    "\n",
    "df_model_test['queryvstitle'] = df_model_test.apply(lambda x: float(x['commons_in_title'])/max((x['len_of_query']),1), axis=1)\n",
    "\n",
    "df_model_test['product_uid'] =df_all_test['product_uid']\n",
    "\n",
    "df_model_test['queryvsdesc'] = df_model_test.apply(lambda x: float(x['commons_in_desc'])/max(x['len_of_query'],1), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_of_query</th>\n",
       "      <th>commons_in_title</th>\n",
       "      <th>commons_in_desc</th>\n",
       "      <th>len_search_term</th>\n",
       "      <th>dist_in_title</th>\n",
       "      <th>dist_in_title1</th>\n",
       "      <th>dist_in_desc</th>\n",
       "      <th>dist_in_desc1</th>\n",
       "      <th>len_search</th>\n",
       "      <th>common_in_desc_pro</th>\n",
       "      <th>queryvstitle</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>queryvsdesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.199950</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.179414</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.079820</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.123923</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.203613</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.188907</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100002</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.254357</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.218599</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100005</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.257484</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.224937</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100005</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len_of_query  commons_in_title  commons_in_desc  len_search_term  \\\n",
       "0             2                 1                1                2   \n",
       "1             2                 1                1                2   \n",
       "2             1                 1                1                1   \n",
       "3             3                 1                1                3   \n",
       "4             2                 2                2                2   \n",
       "\n",
       "   dist_in_title  dist_in_title1  dist_in_desc  dist_in_desc1  len_search  \\\n",
       "0       0.199950        0.153846      0.179414       0.153846          12   \n",
       "1       0.079820        0.200000      0.123923       0.200000           9   \n",
       "2       0.203613        0.400000      0.188907       0.400000           4   \n",
       "3       0.254357        0.117647      0.218599       0.117647          16   \n",
       "4       0.257484        0.142857      0.224937       0.142857          13   \n",
       "\n",
       "   common_in_desc_pro  queryvstitle  product_uid  queryvsdesc  \n",
       "0                   1      0.500000       100001     0.500000  \n",
       "1                   1      0.500000       100001     0.500000  \n",
       "2                   1      1.000000       100002     1.000000  \n",
       "3                   1      0.333333       100005     0.333333  \n",
       "4                   2      1.000000       100005     1.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above briefly shows the features of our work so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using TF IDF method to compute text feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the above methods, we can also use more powerful algorithms to obtain some features.For example, Using TF IDF method to compute the text feature. <br>\n",
    "The TF IDF is a kind of statistical method, the principle is = TF * IDF.  It assesses the importance of a word in a document or a corpus. if a word or phrase in an article has high frequency (TF), and rarely appears in other articles (IDF), we would say this word or phrase has great distinguishing ability. And the TF - IDF values will increase with the word show more times in the document.  It will also decrease with the increase of the number of words appearing in the corpus.<br>\n",
    "For the TFIDF model, we need to build a dictionary using the data set.<br>\n",
    "Here we chose to create our dictionary in the environment of gensim. Then we can calculate that there are 28531 unique words which we use to create a corpus. <br>\n",
    "What is important is that we intend to convert every word into a digit, so that when we are using the tf-idf model, the same word will become the same number,which makes it simpler and clearer. Also since the corpus are generally very big, so we will use an iterator.<br>\n",
    "Then we write a class to sweep up all our corpus and turn them into a simple count of words. This would be a bag of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(28531 unique tokens: ['alon', 'also', 'angl', 'bent', 'coat']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import tokenize\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "#dictionary.\n",
    "### For the TFIDF model, we need to build a thesaurus using the data set.\n",
    "\n",
    "dictionary = Dictionary(list(tokenize(x, errors='ignore')) for x in df_all['all_texts_trans_stemm'].values)\n",
    "print(dictionary)\n",
    "\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for x in df_all['all_texts_trans_stemm'].values:\n",
    "            yield dictionary.doc2bow(list(tokenize(x, errors='ignore')))\n",
    "\n",
    "#\n",
    "corpus = MyCorpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we Training the TFIDF model directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick test to convert 'hello world, good morning' into a number (here's one stopwords, so 3 outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(806, 0.36030376943542936),\n",
       " (2358, 0.33818710980924394),\n",
       " (11067, 0.8693737242920857)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Training the TFIDF model\n",
    "\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corpus)\n",
    "\n",
    "tfidf[dictionary.doc2bow(list(tokenize('hello world, good morning', errors='ignore')))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the large string as the reference vector and the small string with complement of  0 can solve the problem that the size does not match when comparing strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we use TFIDF model to convert text to TFIDF value，and creating a cosine similarity comparison method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.similarities import MatrixSimilarity\n",
    "\n",
    "#### Use TFIDF model to convert text to TFIDF value\n",
    "\n",
    "def to_tfidf(text):\n",
    "    res = tfidf[dictionary.doc2bow(list(tokenize(text, errors='ignore')))]\n",
    "    return res\n",
    "\n",
    "# Creating a cosine similarity comparison method\n",
    "\n",
    "def cos_sim(text1, text2):\n",
    "    tfidf1 = to_tfidf(text1)\n",
    "    tfidf2 = to_tfidf(text2)\n",
    "    index = MatrixSimilarity([tfidf1],num_features=len(dictionary))\n",
    "    sim = index[tfidf2]\n",
    "    return float(sim[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then Calculate TFIDF features on training and test data，we can get the new feature：<br>\n",
    "similarity between search term and product title<br>\n",
    "and similarity between search term and product description<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate TFIDF features on training and test data\n",
    "\n",
    "#Calculate similarity between search term and product title\n",
    "df_model['tfidf_cos_sim_in_title'] = df_all.apply(lambda x: cos_sim(x['search_term_transform_stem'], x['product_title_transform_stem']), axis=1)\n",
    "df_model_test['tfidf_cos_sim_in_title'] = df_all_test.apply(lambda x: cos_sim(x['search_term_transform_stem'], x['product_title_transform_stem']), axis=1)\n",
    "\n",
    "#Calculate similarity between search term and product description\n",
    "df_model['tfidf_cos_sim_in_desc'] = df_all.apply(lambda x: cos_sim(x['search_term_transform_stem'], x['pro_des_trans_stem']), axis=1)\n",
    "df_model_test['tfidf_cos_sim_in_desc'] = df_all_test.apply(lambda x: cos_sim(x['search_term_transform_stem'], x['pro_des_trans_stem']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_of_query</th>\n",
       "      <th>commons_in_title</th>\n",
       "      <th>commons_in_desc</th>\n",
       "      <th>len_search_term</th>\n",
       "      <th>dist_in_title</th>\n",
       "      <th>dist_in_title1</th>\n",
       "      <th>dist_in_desc</th>\n",
       "      <th>dist_in_desc1</th>\n",
       "      <th>len_search</th>\n",
       "      <th>common_in_desc_pro</th>\n",
       "      <th>queryvstitle</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>queryvsdesc</th>\n",
       "      <th>tfidf_cos_sim_in_title</th>\n",
       "      <th>tfidf_cos_sim_in_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075893</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.127041</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113459</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.162306</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.131750</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.345086</td>\n",
       "      <td>0.081922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.264254</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.205404</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.271635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.158553</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.136194</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.271635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len_of_query  commons_in_title  commons_in_desc  len_search_term  \\\n",
       "0             3                 0                1                3   \n",
       "1             3                 1                1                3   \n",
       "2             3                 1                1                3   \n",
       "3             3                 3                3                3   \n",
       "4             5                 3                3                5   \n",
       "\n",
       "   dist_in_title  dist_in_title1  dist_in_desc  dist_in_desc1  len_search  \\\n",
       "0       0.075893        0.117647      0.127041       0.117647          16   \n",
       "1       0.113459        0.125000      0.153707       0.125000          15   \n",
       "2       0.162306        0.125000      0.131750       0.125000          15   \n",
       "3       0.264254        0.105263      0.205404       0.105263          18   \n",
       "4       0.158553        0.074074      0.136194       0.074074          26   \n",
       "\n",
       "   common_in_desc_pro  queryvstitle  product_uid  queryvsdesc  \\\n",
       "0                   0      0.000000       100001     0.333333   \n",
       "1                   1      0.333333       100001     0.333333   \n",
       "2                   1      0.333333       100001     0.333333   \n",
       "3                   3      1.000000       100001     1.000000   \n",
       "4                   3      0.600000       100001     0.600000   \n",
       "\n",
       "   tfidf_cos_sim_in_title  tfidf_cos_sim_in_desc  \n",
       "0                0.000000               0.000000  \n",
       "1                0.000000               0.000000  \n",
       "2                0.345086               0.081922  \n",
       "3                0.861288               0.271635  \n",
       "4                0.861288               0.271635  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Word2Vec method to compute text feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a big difference between w2v and tf-idf. For tf-idf, all we need to know is which word elements are contained in a whole text and then we are all set.<br>\n",
    "\n",
    "But w2v needs to consider the split of the sentence hierarchy, so we can’t use the TF-IDF corpus directly. Here, we need to sort out the sentences/words first.<br>\n",
    "First, we import nltk which also comes with a powerful sentence splitter.<br>\n",
    "Next  we make long text into list of sentences, then turn sentences into list of words.<br>\n",
    "Since the sentences do not need these hierarchies, they are all flat, so we gave flatten to list of lists.<br>\n",
    "We divide up the words in the sentence. You can use the tokenizer just from Gensim or the word_tokenizer from NLTK.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zwl/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#1）nltk also comes with a powerful sentence splitter. [Call tool]\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#2）We first make long text into list of sentences, and then turn sentences into list of words: [text-> sentence]\n",
    "sentences = [tokenizer.tokenize(x) for x in df_all['all_texts_trans_stemm'].values]\n",
    "\n",
    "#3）We gave flatten to list of lists. [Sentence-> flatten]\n",
    "sentences = [y for x in sentences for y in x] #\n",
    "\n",
    "#4）We divide up the words in the sentence. You can use the tokenizer just from Gensim or the word_tokenizer from nltk[sentence -> words]\n",
    "from nltk.tokenize import word_tokenize\n",
    "w2v_corpus = [word_tokenize(x) for x in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(  'data.pkl','wb' ) as f:\n",
    "# pickle.dump(w2v_corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And train our predictive database into word vectors, at this point, each word can read out its w2v vector like a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "#5） Train our predictive database into word vectors [Words-> Training Corpus Model]\n",
    "# model = Word2Vec(w2v_corpus, size=128, window=5, min_count=5, workers=4)\n",
    "model = Word2Vec.load('w2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.word2vec.Word2Vec at 0x7f376d175668>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like TFiDF, we can turn the columns of textual into w2v vectors. <br>\n",
    "The difference here is that TFiDF is for every sentence, while w2v is for every word. Therefore, we can average the w2v vector of a sentence and calculate the similarity of word vectors between texts as the average vector then we calculate the similarity of word vectors between texts.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.word2vec.Word2VecVocab object at 0x7f3766c97358>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#6) You can get a vector of each word, but each sentence is composed of multiple words, and each word vector is averaged,\n",
    "vocab = model.vocabulary\n",
    "print(vocab)\n",
    "\n",
    "### Get word vector for text\n",
    "def get_vector(text):\n",
    "    res = np.zeros([128])\n",
    "    count = 0\n",
    "    for word in word_tokenize(text):\n",
    "        res += model[word]\n",
    "        count+=1\n",
    "    return res/max(count,1)\n",
    "\n",
    "from scipy import spatial\n",
    "### Calculate the similarity of word vectors between texts\n",
    "def w2v_cos_sim(text1, text2):\n",
    "    try:\n",
    "        w2v1 = get_vector(text1)\n",
    "        w2v2 = get_vector(text2)\n",
    "        sim = 1 - spatial.distance.cosine(w2v1, w2v2)\n",
    "        return float(sim)\n",
    "    except:\n",
    "        return float(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "last,we use the previous word vector feature function to extract training text features and test text features<br>\n",
    "Then we can get a numblized large table with many features created by our own as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zwl/miniconda3/envs/python36/lib/python3.6/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "### feature about word2vec.\n",
    "\n",
    "### Apply the previous word vector \n",
    "### feature function to extract training text features and test text features\n",
    "\n",
    "df_model['w2v_cos_sim_in_title'] = df_all.apply(lambda x: w2v_cos_sim(x['search_term_transform_stem'], x['product_title_transform_stem']), axis=1)\n",
    "df_model['w2v_cos_sim_in_desc'] = df_all.apply(lambda x: w2v_cos_sim(x['search_term_transform_stem'], x['pro_des_trans_stem']), axis=1)\n",
    "\n",
    "df_model_test['w2v_cos_sim_in_title'] = df_all_test.apply(lambda x: w2v_cos_sim(x['search_term_transform_stem'], x['product_title_transform_stem']), axis=1)\n",
    "df_model_test['w2v_cos_sim_in_desc'] = df_all_test.apply(lambda x: w2v_cos_sim(x['search_term_transform_stem'], x['pro_des_trans_stem']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>len_of_query</th>\n",
       "      <th>commons_in_title</th>\n",
       "      <th>commons_in_desc</th>\n",
       "      <th>len_search_term</th>\n",
       "      <th>dist_in_title</th>\n",
       "      <th>dist_in_title1</th>\n",
       "      <th>dist_in_desc</th>\n",
       "      <th>dist_in_desc1</th>\n",
       "      <th>len_search</th>\n",
       "      <th>common_in_desc_pro</th>\n",
       "      <th>queryvstitle</th>\n",
       "      <th>product_uid</th>\n",
       "      <th>queryvsdesc</th>\n",
       "      <th>tfidf_cos_sim_in_title</th>\n",
       "      <th>tfidf_cos_sim_in_desc</th>\n",
       "      <th>w2v_cos_sim_in_title</th>\n",
       "      <th>w2v_cos_sim_in_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.075893</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.127041</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.113459</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.153707</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.162306</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.131750</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.345086</td>\n",
       "      <td>0.081922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.264254</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.205404</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100001</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.271635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0.158553</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.136194</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>100001</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.861288</td>\n",
       "      <td>0.271635</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   len_of_query  commons_in_title  commons_in_desc  len_search_term  \\\n",
       "0             3                 0                1                3   \n",
       "1             3                 1                1                3   \n",
       "2             3                 1                1                3   \n",
       "3             3                 3                3                3   \n",
       "4             5                 3                3                5   \n",
       "\n",
       "   dist_in_title  dist_in_title1  dist_in_desc  dist_in_desc1  len_search  \\\n",
       "0       0.075893        0.117647      0.127041       0.117647          16   \n",
       "1       0.113459        0.125000      0.153707       0.125000          15   \n",
       "2       0.162306        0.125000      0.131750       0.125000          15   \n",
       "3       0.264254        0.105263      0.205404       0.105263          18   \n",
       "4       0.158553        0.074074      0.136194       0.074074          26   \n",
       "\n",
       "   common_in_desc_pro  queryvstitle  product_uid  queryvsdesc  \\\n",
       "0                   0      0.000000       100001     0.333333   \n",
       "1                   1      0.333333       100001     0.333333   \n",
       "2                   1      0.333333       100001     0.333333   \n",
       "3                   3      1.000000       100001     1.000000   \n",
       "4                   3      0.600000       100001     0.600000   \n",
       "\n",
       "   tfidf_cos_sim_in_title  tfidf_cos_sim_in_desc  w2v_cos_sim_in_title  \\\n",
       "0                0.000000               0.000000                   0.0   \n",
       "1                0.000000               0.000000                   0.0   \n",
       "2                0.345086               0.081922                   0.0   \n",
       "3                0.861288               0.271635                   0.0   \n",
       "4                0.861288               0.271635                   0.0   \n",
       "\n",
       "   w2v_cos_sim_in_desc  \n",
       "0                  0.0  \n",
       "1                  0.0  \n",
       "2                  0.0  \n",
       "3                  0.0  \n",
       "4                  0.0  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GBDT method to train model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we chose Gradient Boosting Decision Tree to do the training since it has good performance in ensembling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training dataset, we can get train feature x and train label y, then get the test feature for testing dataset.<br>\n",
    "By the way, since there is no relevance direct result in training data set, in order to ensure the effectiveness of our method, we first split the train data set, we divide train set and test set using 0.2 ratio. And then directly use GradientBoostingRegressor to train the  GBDT model using training dataset.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For training data, get train feature x and train label y.\n",
    "\n",
    "y = df_all['relevance'].values\n",
    "x = df_model.values\n",
    "\n",
    "#### Get test feature.\n",
    "\n",
    "test_x = df_model_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:  (74067, 17)\n",
      "y:  (74067,)\n",
      "test x... (166693, 17)\n"
     ]
    }
   ],
   "source": [
    "print('x: ', x.shape)\n",
    "print('y: ', y.shape)\n",
    "print('test x...', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59253,)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### divide train set and test set using 0.2 ratio../\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### gbdt model..\n",
    "\n",
    "gbdt = GradientBoostingRegressor()\n",
    "gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n",
       "                          learning_rate=0.1, loss='ls', max_depth=3,\n",
       "                          max_features=None, max_leaf_nodes=None,\n",
       "                          min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                          min_samples_leaf=1, min_samples_split=2,\n",
       "                          min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                          n_iter_no_change=None, presort='auto',\n",
       "                          random_state=None, subsample=1.0, tol=0.0001,\n",
       "                          validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### train gbdt model using train data.\n",
    "gbdt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and predict and compute MSE on test set of the original train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22196498084458471"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### predict and compute MSE on test set.\n",
    "\n",
    "y_pred = gbdt.predict(x_test)\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "### compute MSE on test set.\n",
    "mse(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The correlation prediction error is 0.22, the performance is relatively good, and the error is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the trained model to predict test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(166693,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.86558007, 2.04259776, 2.25202692, ..., 2.3285694 , 2.53979406,\n",
       "       2.31565136])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y_pred = gbdt.predict(test_x)\n",
    "print(test_y_pred.shape)\n",
    "test_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166693, 4)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#### Save csv file about test data.\n",
    "from pandas import DataFrame as df\n",
    "res = pd.concat( [df(df_test), df(test_y_pred)], axis=1)\n",
    "res.to_csv('predict_res.csv', header=None)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
